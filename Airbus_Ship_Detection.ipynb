{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Airbus Ship Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNKsykTsz6GBl7tNKTVwKWj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mziad97/Airbus-ship-semantic-segmentation/blob/main/Airbus_Ship_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBqe0-KpoG0O"
      },
      "source": [
        "## for viz\n",
        "# def apply_mask(image, mask):\n",
        "#     for x, y in mask:\n",
        "#         image[x, y, [0, 1]] = 255\n",
        "#     return image\n",
        "\n",
        "\n",
        "# load_img = lambda filename: np.array(PIL.Image.open(f\"./train_v2/{filename}\"))\n",
        "\n",
        "# img = load_img(segments.loc[0, 'ImageId'])\n",
        "# mask_pixels = rle_to_pixels(segments.loc[0, 'EncodedPixels'])\n",
        "# img = apply_mask(img, mask_pixels)\n",
        "# plt.imshow(img);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUZEORKFIt8J"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4LVhjZOSZxG"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tw4l3KxSDcg"
      },
      "source": [
        "## UNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGQXFQDXOLJl"
      },
      "source": [
        "# Encoder\n",
        "\n",
        "def conv2d_block(input, n_filters, kernel_size=3):\n",
        "  x = input\n",
        "  for i in range(2):\n",
        "    x = tf.keras.layers.Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), \n",
        "                               kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):\n",
        "\n",
        "  f = conv2d_block(inputs, n_filters=n_filters)\n",
        "  p = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(f)\n",
        "  p = tf.keras.layers.Dropout(0.3)(p)\n",
        "\n",
        "  return f, p\n",
        "\n",
        "def encoder(inputs):\n",
        "  f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3)\n",
        "  f2, p2 = encoder_block(p1, n_filters= 128, pool_size=(2,2), dropout=0.3)\n",
        "  f3, p3 = encoder_block(p2, n_filters= 256, pool_size=(2,2), dropout=0.3)\n",
        "  f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2,2), dropout=0.3)\n",
        "\n",
        "  return p4, (f1,f2,f3,f4)\n",
        "\n",
        "def bottleneck(inputs):\n",
        "  bottle_neck = conv2d_block(inputs, n_filters=1024)\n",
        "  return bottle_neck"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtt03WxxWIWb"
      },
      "source": [
        "# Decoder Utilities\n",
        "\n",
        "def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=3, dropout=0.3):\n",
        "  '''\n",
        "  defines the one decoder block of the UNet\n",
        "\n",
        "  Args:\n",
        "    inputs (tensor) -- batch of input features\n",
        "    conv_output (tensor) -- features from an encoder block\n",
        "    n_filters (int) -- number of filters\n",
        "    kernel_size (int) -- kernel size\n",
        "    strides (int) -- strides for the deconvolution/upsampling\n",
        "    padding (string) -- \"same\" or \"valid\", tells if shape will be preserved by zero padding\n",
        "\n",
        "  Returns:\n",
        "    c (tensor) -- output features of the decoder block\n",
        "  '''\n",
        "  u = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides = strides, padding = 'same')(inputs)\n",
        "  c = tf.keras.layers.concatenate([u, conv_output])\n",
        "  c = tf.keras.layers.Dropout(dropout)(c)\n",
        "  c = conv2d_block(c, n_filters, kernel_size=3)\n",
        "\n",
        "  return c\n",
        "\n",
        "\n",
        "def decoder(inputs, convs, output_channels):\n",
        "  '''\n",
        "  Defines the decoder of the UNet chaining together 4 decoder blocks. \n",
        "  \n",
        "  Args:\n",
        "    inputs (tensor) -- batch of input features\n",
        "    convs (tuple) -- features from the encoder blocks\n",
        "    output_channels (int) -- number of classes in the label map\n",
        "\n",
        "  Returns:\n",
        "    outputs (tensor) -- the pixel wise label map of the image\n",
        "  '''\n",
        "  \n",
        "  f1, f2, f3, f4 = convs\n",
        "\n",
        "  c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c7 = decoder_block(c6, f3, n_filters=256, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c8 = decoder_block(c7, f2, n_filters=128, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "  c9 = decoder_block(c8, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2D(output_channels, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  return outputs"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA4pSdsEbvtS"
      },
      "source": [
        "OUTPUT_CHANNELS = 1\n",
        "\n",
        "def unet():\n",
        "  '''\n",
        "  Defines the UNet by connecting the encoder, bottleneck and decoder.\n",
        "  '''\n",
        "\n",
        "  # specify the input shape\n",
        "  inputs = tf.keras.layers.Input(shape=(128, 128,3,))\n",
        "\n",
        "  # feed the inputs to the encoder\n",
        "  encoder_output, convs = encoder(inputs)\n",
        "\n",
        "  # feed the encoder output to the bottleneck\n",
        "  bottle_neck = bottleneck(encoder_output)\n",
        "\n",
        "  # feed the bottleneck and encoder block outputs to the decoder\n",
        "  # specify the number of classes via the `output_channels` argument\n",
        "  outputs = decoder(bottle_neck, convs, output_channels=OUTPUT_CHANNELS)\n",
        "  \n",
        "  # create the model\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5owfJTeddKi6"
      },
      "source": [
        "model = unet()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGoeuePhRnHT"
      },
      "source": [
        "## Get the data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kwgntYIE85f"
      },
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "! mkdir ~/.kaggle"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjJqALL2G0pe"
      },
      "source": [
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bKddjCrHCHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "466680eb-17be-4784-8002-5bfe428f8b5a"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 2.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=28f82cbb474e2e2bf0c4f5762422b1d07e97116af18af12e66735cd136159a62\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCFdC4L2sWiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209dea78-bf1c-4008-b9c3-1dc20e92ff30"
      },
      "source": [
        "if ('train_v2' not in os.listdir('.')):\n",
        "  ! kaggle competitions download -c airbus-ship-detection\n",
        "\n",
        "  with ZipFile('airbus-ship-detection.zip', 'r') as zipObj:\n",
        "    zipObj.extract('train_ship_segmentations_v2.csv')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading airbus-ship-detection.zip to /content\n",
            "100% 28.6G/28.6G [09:39<00:00, 61.8MB/s]\n",
            "100% 28.6G/28.6G [09:42<00:00, 52.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDGI2zKITzYj"
      },
      "source": [
        "segments = pd.read_csv('train_ship_segmentations_v2.csv', index_col=0).dropna().reset_index()\n",
        "segments['ImageId'] = segments['ImageId'].map(lambda filename: \"train_v2/\" + filename)\n",
        "\n",
        "\n",
        "with ZipFile('airbus-ship-detection.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "  for file in segments['ImageId'].values:\n",
        "      zipObj.extract(file)\n",
        "\n",
        "! rm airbus-ship-detection.zip"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcmzMEHNtJGT"
      },
      "source": [
        "# read segments, join images containing multiple ships\n",
        "segments = segments.groupby(\"ImageId\")[['EncodedPixels']].agg(lambda rle_codes: ' '.join(rle_codes)).reset_index()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q__t7mqH2xNX"
      },
      "source": [
        "train_paths, test_paths = train_test_split(segments, train_size=0.8, shuffle=True, random_state=0)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weWcTo5d2xKS",
        "outputId": "bb3fc903-0246-4917-a27f-eee4300708b9"
      },
      "source": [
        "print(f\"The number of train set: {len(train_paths)}\")\n",
        "print(f\"The number of test set: {len(test_paths)}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of train set: 34044\n",
            "The number of test set: 8512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MA89fQKo-7b"
      },
      "source": [
        "## Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRpHTi-fbxDT"
      },
      "source": [
        "# def to_mask(rle):\n",
        "#   pixels = rle_to_pixels(rle)\n",
        "#   temp_var = tf.Variable(initial_value=[0]*589824, dtype=tf.uint8)\n",
        "#   # tf.Variable(tf.zeros(shape=(589824)) )\n",
        "#   tf.compat.v1.scatter_update(ref=temp_var,\n",
        "#                             indices=pixels,\n",
        "#                             updates=1)\n",
        "#   temp_var = tf.reshape(temp_var, (768,768))\n",
        "#   return tf.transpose(tf.expand_dims(temp_var, 0)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppd6yi8obmbZ"
      },
      "source": [
        "def rle_to_pixels(rle_code):\n",
        "    rle_code = tf.strings.to_number( tf.strings.split(rle_code), tf.int64 )\n",
        "    # rle_code = [int(i) for i in tf.strings.split(rle_code)]\n",
        "    output = tf.map_fn(fn=lambda start_len: tf.range(start=start_len[0], limit=start_len[0] + start_len[1]) ,\n",
        "             elems=tf.stack([rle_code[0:-1:2], rle_code[1::2]], axis=1), \n",
        "             fn_output_signature=tf.RaggedTensorSpec(shape=[None], dtype=tf.int64) )\n",
        "    \n",
        "    pixels = tf.stack( tf.map_fn(fn=lambda x: (x % 768, x // 768), elems=output.flat_values, \n",
        "                                 fn_output_signature= (tf.int64, tf.int64)), axis=1)\n",
        "    \n",
        "    # pixels = [(pixel_position % 768, pixel_position // 768) \n",
        "    #              for start, length in tf.stack([temp_rle[0:-1:2], temp_rle[1::2]], axis=1) \n",
        "    #              for pixel_position in tf.range(start, start + length)]\n",
        "    \n",
        "    return pixels\n",
        "    # output.flat_values"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPgBhFw-bncd"
      },
      "source": [
        "def pixels_to_mask(pixels):\n",
        "  # temp_var = tf.Variable(initial_value=[0]*589824, dtype=tf.uint8)\n",
        "  # tf.Variable(tf.zeros(shape=(589824)) )\n",
        "  \n",
        "  # temp_var = tf.sparse.SparseTensor(indices=pixels, values=tf.ones(shape=len(pixels), dtype=tf.uint8), dense_shape=(768, 768))\n",
        "\n",
        "  # slices = tf.IndexedSlices(1, indices=pixels)\n",
        "  # temp_var.scatter_update(slices) \n",
        "  # temp_var = tf.reshape(temp_var, (768,768))\n",
        "  # tf.transpose(tf.expand_dims(temp_var, 0))\n",
        "\n",
        "  return tf.expand_dims( tf.sparse.to_dense( tf.sparse.reorder( tf.sparse.SparseTensor(indices=pixels, \n",
        "                                       values=tf.ones(shape=len(pixels), dtype=tf.uint8), \n",
        "                                           dense_shape=(768, 768)) )), axis=2 )"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYHEBfl1T-Sd"
      },
      "source": [
        "def normalize(input_image):\n",
        "  \"\"\"normalizes the input image pixel values to be [0,1] \"\"\"\n",
        "\n",
        "  input_image = tf.cast(input_image, tf.float32)\n",
        "  input_image /= 255.0\n",
        "  return input_image"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV2quT5_k2Jg"
      },
      "source": [
        "def load_image(datapoint):\n",
        "  \"\"\"\n",
        "  return a resized and normalized pair of image and mask\n",
        "  args\n",
        "    datapoint: a single image and its corresponding segmentation mask\n",
        "\n",
        "  1. load the image from its path, decode it to jpeg, normalize it to [0,1]\n",
        "  2. decode the run-length encoding to pixels, then project the mask onto canvas with same size as image\n",
        "  3. resize both the image and segmentation mask, to math the input size of the network i.e (128,128)\n",
        "  \"\"\"\n",
        "\n",
        "  input_image = tf.io.read_file(datapoint[0])\n",
        "  input_image = tf.image.decode_jpeg(input_image, channels=3)\n",
        "  input_image = tf.image.resize(input_image, (128, 128), method='nearest')\n",
        "  \n",
        "  rle =  rle_to_pixels(datapoint[1])\n",
        "  mask = pixels_to_mask(rle)\n",
        "  input_mask = tf.image.resize(mask, (128, 128), method='nearest')\n",
        "  \n",
        "  input_image = normalize(input_image)\n",
        "\n",
        "  return input_image, input_mask"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQNE4jTtk8kl"
      },
      "source": [
        "train_paths_dataset = tf.data.Dataset.from_tensor_slices(train_paths)\n",
        "test_paths_dataset = tf.data.Dataset.from_tensor_slices(test_paths)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBDBKC_2mz0o"
      },
      "source": [
        "train = train_paths_dataset.map(load_image)\n",
        "test = test_paths_dataset.map(load_image)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mioekO7KcQAN"
      },
      "source": [
        "## Training The UNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hz3VsCJE1w2"
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# shuffle and group the train set into batches\n",
        "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "# do a prefetch to optimize processing\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# group the test set into batches\n",
        "test_dataset = test.batch(BATCH_SIZE)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwlubMwkkD7M"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNZJqQ1d0sjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "1f0af561-28ab-48c9-d8d1-3606bcd7bf69"
      },
      "source": [
        "# configure the training parameters and train the model\n",
        "\n",
        "TRAIN_LENGTH = len(train_paths)\n",
        "EPOCHS = 2\n",
        "VAL_SUBSPLITS = 5\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "VALIDATION_STEPS = len(test_paths) // BATCH_SIZE // VAL_SUBSPLITS\n",
        "\n",
        "\n",
        "model_history = model.fit(train_dataset, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=test_dataset)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-f59eb5e60379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVALIDATION_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                           validation_data=test_dataset)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[34] = [0,768] is out of bounds: need 0 <= index < [768,768]\n\t [[{{node SparseToDense}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_323176]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG7YwEWkrMQG"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkW5tO5qlqOs"
      },
      "source": [
        "def dice_metric(y_true, y_pred):\n",
        "\n",
        "  smoothening_factor = 0.00001\n",
        "    \n",
        "  intersection = np.sum((y_pred == i) * (y_true == i))\n",
        "  y_true_area = np.sum((y_true == i))\n",
        "  y_pred_area = np.sum((y_pred == i))\n",
        "  combined_area = y_true_area + y_pred_area\n",
        "\n",
        "  \n",
        "  dice_score =  2 * ((intersection + smoothening_factor) / (combined_area + smoothening_factor))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz1J4aXM_4KL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnfwQg42_8TM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9olWD_NNCHIP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}